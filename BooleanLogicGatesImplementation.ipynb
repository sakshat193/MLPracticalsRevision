{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391bd7fa",
   "metadata": {},
   "source": [
    "# Q1.1: Implement Boolean Logic Gates Using Perceptron\n",
    "\n",
    "Implement AND, OR, NOT, NAND, NOR using a single-layer perceptron (step activation).\n",
    "\n",
    "**Exam outputs:** truth tables + final weights/bias for each gate. For XOR, show why a single perceptron fails and implement a simple MLP-style solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212bce0",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32950207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step activation function\n",
    "def step(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "# Perceptron training function\n",
    "def train_perceptron(X, y, lr=0.1, epochs=20):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            z = np.dot(X[i], w) + b\n",
    "            y_pred = step(z)\n",
    "            error = y[i] - y_pred\n",
    "            w += lr * error * X[i]\n",
    "            b += lr * error\n",
    "    return w, b\n",
    "\n",
    "# Predict function\n",
    "def predict(X, w, b):\n",
    "    return [step(np.dot(x, w) + b) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabd542",
   "metadata": {},
   "source": [
    "## Step 2: AND Gate Implementation\n",
    "\n",
    "### Define Input and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709d5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs: x1, x2\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17111a9f",
   "metadata": {},
   "source": [
    "### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d395856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate\n",
      "Final Weights: [0.2 0.1]\n",
      "Final Bias: -0.20000000000000004\n",
      "\n",
      "Truth Table:\n",
      "x1  x2  | Predicted\n",
      "-------------------\n",
      " 0   0  |     0\n",
      " 0   1  |     0\n",
      " 1   0  |     0\n",
      " 1   1  |     1\n"
     ]
    }
   ],
   "source": [
    "w_and, b_and = train_perceptron(X, y_and)\n",
    "y_pred_and = predict(X, w_and, b_and)\n",
    "\n",
    "print(\"AND Gate\")\n",
    "print(f\"Final Weights: {w_and}\")\n",
    "print(f\"Final Bias: {b_and}\")\n",
    "print(\"\\nTruth Table:\")\n",
    "print(\"x1  x2  | Predicted\")\n",
    "print(\"-------------------\")\n",
    "for i in range(len(X)):\n",
    "    print(f\" {X[i][0]}   {X[i][1]}  |     {y_pred_and[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f526a0d",
   "metadata": {},
   "source": [
    "## Step 3: OR Gate Implementation\n",
    "\n",
    "### Define Target and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cdd33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR Gate\n",
      "Final Weights: [0.1 0.1]\n",
      "Final Bias: -0.1\n",
      "\n",
      "Truth Table:\n",
      "x1  x2  | Predicted\n",
      "-------------------\n",
      " 0   0  |     0\n",
      " 0   1  |     1\n",
      " 1   0  |     1\n",
      " 1   1  |     1\n"
     ]
    }
   ],
   "source": [
    "y_or = np.array([0, 1, 1, 1])\n",
    "w_or, b_or = train_perceptron(X, y_or)\n",
    "y_pred_or = predict(X, w_or, b_or)\n",
    "\n",
    "print(\"OR Gate\")\n",
    "print(f\"Final Weights: {w_or}\")\n",
    "print(f\"Final Bias: {b_or}\")\n",
    "print(\"\\nTruth Table:\")\n",
    "print(\"x1  x2  | Predicted\")\n",
    "print(\"-------------------\")\n",
    "for i in range(len(X)):\n",
    "    print(f\" {X[i][0]}   {X[i][1]}  |     {y_pred_or[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f0f00",
   "metadata": {},
   "source": [
    "## Step 4: NOT Gate Implementation\n",
    "\n",
    "### Define Input and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "955bfaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT Gate\n",
      "Final Weight: [-0.1]\n",
      "Final Bias: 0.0\n",
      "\n",
      "Truth Table:\n",
      " x  | Predicted\n",
      "--------------\n",
      " 0  |     1\n",
      " 1  |     0\n"
     ]
    }
   ],
   "source": [
    "# Single input for NOT gate\n",
    "X_not = np.array([[0], [1]])\n",
    "y_not = np.array([1, 0])\n",
    "\n",
    "w_not, b_not = train_perceptron(X_not, y_not)\n",
    "y_pred_not = predict(X_not, w_not, b_not)\n",
    "\n",
    "print(\"NOT Gate\")\n",
    "print(f\"Final Weight: {w_not}\")\n",
    "print(f\"Final Bias: {b_not}\")\n",
    "print(\"\\nTruth Table:\")\n",
    "print(\" x  | Predicted\")\n",
    "print(\"--------------\")\n",
    "for i in range(len(X_not)):\n",
    "    print(f\" {X_not[i][0]}  |     {y_pred_not[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141fa012",
   "metadata": {},
   "source": [
    "## Step 5: NAND Gate Implementation\n",
    "\n",
    "### Define Target and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3daad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND Gate\n",
      "Final Weights: [-0.2 -0.1]\n",
      "Final Bias: 0.2\n",
      "\n",
      "Truth Table:\n",
      "x1  x2  | Predicted\n",
      "-------------------\n",
      " 0   0  |     1\n",
      " 0   1  |     1\n",
      " 1   0  |     1\n",
      " 1   1  |     0\n"
     ]
    }
   ],
   "source": [
    "y_nand = np.array([1, 1, 1, 0])\n",
    "w_nand, b_nand = train_perceptron(X, y_nand)\n",
    "y_pred_nand = predict(X, w_nand, b_nand)\n",
    "\n",
    "print(\"NAND Gate\")\n",
    "print(f\"Final Weights: {w_nand}\")\n",
    "print(f\"Final Bias: {b_nand}\")\n",
    "print(\"\\nTruth Table:\")\n",
    "print(\"x1  x2  | Predicted\")\n",
    "print(\"-------------------\")\n",
    "for i in range(len(X)):\n",
    "    print(f\" {X[i][0]}   {X[i][1]}  |     {y_pred_nand[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcc98f",
   "metadata": {},
   "source": [
    "## Step 6: NOR Gate Implementation\n",
    "\n",
    "### Define Target and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "682cca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOR Gate\n",
      "Final Weights: [-0.1 -0.1]\n",
      "Final Bias: 0.0\n",
      "\n",
      "Truth Table:\n",
      "x1  x2  | Predicted\n",
      "-------------------\n",
      " 0   0  |     1\n",
      " 0   1  |     0\n",
      " 1   0  |     0\n",
      " 1   1  |     0\n"
     ]
    }
   ],
   "source": [
    "y_nor = np.array([1, 0, 0, 0])\n",
    "w_nor, b_nor = train_perceptron(X, y_nor)\n",
    "y_pred_nor = predict(X, w_nor, b_nor)\n",
    "\n",
    "print(\"NOR Gate\")\n",
    "print(f\"Final Weights: {w_nor}\")\n",
    "print(f\"Final Bias: {b_nor}\")\n",
    "print(\"\\nTruth Table:\")\n",
    "print(\"x1  x2  | Predicted\")\n",
    "print(\"-------------------\")\n",
    "for i in range(len(X)):\n",
    "    print(f\" {X[i][0]}   {X[i][1]}  |     {y_pred_nor[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922389dd",
   "metadata": {},
   "source": [
    "## Step 7: XOR Gate Implementation (Will Fail)\n",
    "\n",
    "### Attempt Training XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0105fec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Gate (Single Layer Perceptron - WILL FAIL)\n",
      "Final Weights: [-0.1  0. ]\n",
      "Final Bias: 0.0\n",
      "\n",
      "Truth Table:\n",
      "x1  x2  | Expected | Predicted\n",
      "---------------------------------\n",
      " 0   0  |    0     |     1\n",
      " 0   1  |    1     |     1\n",
      " 1   0  |    1     |     0\n",
      " 1   1  |    0     |     0\n"
     ]
    }
   ],
   "source": [
    "y_xor = np.array([0, 1, 1, 0])\n",
    "w_xor, b_xor = train_perceptron(X, y_xor, epochs=100)\n",
    "y_pred_xor = predict(X, w_xor, b_xor)\n",
    "\n",
    "print(\"XOR Gate (Single Layer Perceptron - WILL FAIL)\")\n",
    "print(f\"Final Weights: {w_xor}\")\n",
    "print(f\"Final Bias: {b_xor}\")\n",
    "print(\"\\nTruth Table:\")\n",
    "print(\"x1  x2  | Expected | Predicted\")\n",
    "print(\"---------------------------------\")\n",
    "for i in range(len(X)):\n",
    "    print(f\" {X[i][0]}   {X[i][1]}  |    {y_xor[i]}     |     {y_pred_xor[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf41e3",
   "metadata": {},
   "source": [
    "## Step 8: XOR Explanation\n",
    "\n",
    "### Why XOR Fails with Single-Layer Perceptron\n",
    "\n",
    "**Critical Viva Point:**\n",
    "\n",
    "XOR is **NOT linearly separable**. A single-layer perceptron can only learn linear decision boundaries (a straight line in 2D space). \n",
    "\n",
    "The XOR problem requires a non-linear decision boundary that cannot be represented by a single line:\n",
    "- Points (0,0) and (1,1) should output 0\n",
    "- Points (0,1) and (1,0) should output 1\n",
    "\n",
    "There is no single straight line that can separate these two classes.\n",
    "\n",
    "**Solution:** Use a Multi-Layer Perceptron (MLP) with at least one hidden layer to introduce non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae0800a",
   "metadata": {},
   "source": [
    "### XOR Solution Using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb32ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XOR using Multi-Layer Perceptron (NAND + OR + AND):\n",
      "x1  x2  | Predicted\n",
      "-------------------\n",
      " 0   0  |     0\n",
      " 0   1  |     1\n",
      " 1   0  |     1\n",
      " 1   1  |     0\n"
     ]
    }
   ],
   "source": [
    "# Simple MLP for XOR using combination of NAND, OR, AND gates\n",
    "def xor_mlp(x1, x2):\n",
    "    # Layer 1: NAND and OR gates\n",
    "    nand_out = step(np.dot([x1, x2], w_nand) + b_nand)\n",
    "    or_out = step(np.dot([x1, x2], w_or) + b_or)\n",
    "    # Layer 2: AND gate\n",
    "    xor_out = step(np.dot([nand_out, or_out], w_and) + b_and)\n",
    "    return xor_out\n",
    "\n",
    "print(\"\\nXOR using Multi-Layer Perceptron (NAND + OR + AND):\")\n",
    "print(\"x1  x2  | Predicted\")\n",
    "print(\"-------------------\")\n",
    "for i in range(len(X)):\n",
    "    result = xor_mlp(X[i][0], X[i][1])\n",
    "    print(f\" {X[i][0]}   {X[i][1]}  |     {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
