{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05189e67",
   "metadata": {},
   "source": [
    "# Q6.2: Manual Backpropagation (Step-by-Step Calculation)\n",
    "\n",
    "Work through forward pass, loss (MSE), gradients, and one update step for a small network with linear activation.\n",
    "\n",
    "**Exam outputs:** intermediate values (z, a), gradients (dW/db), and updated weights showing loss reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39fb7e",
   "metadata": {},
   "source": [
    "## Step 1: Define Network Architecture\n",
    "\n",
    "### Simple 2-2-1 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ecc217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X: [[0.5 0.1]]\n",
      "\n",
      "Weights W1:\n",
      " [[0.15 0.25]\n",
      " [0.2  0.3 ]]\n",
      "Bias b1: [[0.35 0.35]]\n",
      "\n",
      "Weights W2:\n",
      " [[0.4 ]\n",
      " [0.45]]\n",
      "Bias b2: [[0.6]]\n",
      "\n",
      "Target y: [[0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input\n",
    "X = np.array([[0.5, 0.1]])\n",
    "\n",
    "# Weights layer 1 (2 inputs → 2 hidden)\n",
    "W1 = np.array([[0.15, 0.25],\n",
    "               [0.20, 0.30]])\n",
    "b1 = np.array([[0.35, 0.35]])\n",
    "\n",
    "# Weights layer 2 (2 hidden → 1 output)\n",
    "W2 = np.array([[0.40],\n",
    "               [0.45]])\n",
    "b2 = np.array([[0.60]])\n",
    "\n",
    "# Target\n",
    "y_true = np.array([[0.5]])\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.5\n",
    "\n",
    "print(\"Input X:\", X)\n",
    "print(\"\\nWeights W1:\\n\", W1)\n",
    "print(\"Bias b1:\", b1)\n",
    "print(\"\\nWeights W2:\\n\", W2)\n",
    "print(\"Bias b2:\", b2)\n",
    "print(\"\\nTarget y:\", y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cada58",
   "metadata": {},
   "source": [
    "## Step 2: Forward Propagation (Linear Activation)\n",
    "\n",
    "### Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e47111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer z1: [[0.445 0.505]]\n",
      "Hidden layer a1 (linear): [[0.445 0.505]]\n"
     ]
    }
   ],
   "source": [
    "# Hidden layer calculation\n",
    "z1 = np.dot(X, W1) + b1\n",
    "a1 = z1  # Linear activation (a = z)\n",
    "\n",
    "print(\"Hidden layer z1:\", z1)\n",
    "print(\"Hidden layer a1 (linear):\", a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acd95e9",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output z2: [[1.00525]]\n",
      "Output a2 (prediction): [[1.00525]]\n"
     ]
    }
   ],
   "source": [
    "# Output layer calculation\n",
    "z2 = np.dot(a1, W2) + b2\n",
    "a2 = z2  # Linear activation\n",
    "\n",
    "print(\"Output z2:\", z2)\n",
    "print(\"Output a2 (prediction):\", a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7787db15",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Loss (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (MSE): 0.127639\n",
      "Error: -0.505250\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error\n",
    "loss = 0.5 * (y_true - a2) ** 2\n",
    "\n",
    "print(f\"Loss (MSE): {loss[0,0]:.6f}\")\n",
    "print(f\"Error: {(y_true - a2)[0,0]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d87add",
   "metadata": {},
   "source": [
    "## Step 4: Backward Propagation - Output Layer\n",
    "\n",
    "### Calculate Gradients for W2 and b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541dee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/da2: [[0.50525]]\n",
      "dL/dz2: [[0.50525]]\n",
      "\n",
      "dL/dW2:\n",
      " [[0.22483625]\n",
      " [0.25515125]]\n",
      "dL/db2: [[0.50525]]\n"
     ]
    }
   ],
   "source": [
    "# Gradient of loss w.r.t output\n",
    "dL_da2 = -(y_true - a2)\n",
    "\n",
    "# For linear activation: da2/dz2 = 1\n",
    "dL_dz2 = dL_da2 * 1\n",
    "\n",
    "# Gradient for W2: dL/dW2 = a1^T · dL/dz2\n",
    "dL_dW2 = np.dot(a1.T, dL_dz2)\n",
    "\n",
    "# Gradient for b2: dL/db2 = dL/dz2\n",
    "dL_db2 = dL_dz2\n",
    "\n",
    "print(\"dL/da2:\", dL_da2)\n",
    "print(\"dL/dz2:\", dL_dz2)\n",
    "print(\"\\ndL/dW2:\\n\", dL_dW2)\n",
    "print(\"dL/db2:\", dL_db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7b6a9",
   "metadata": {},
   "source": [
    "## Step 5: Backward Propagation - Hidden Layer\n",
    "\n",
    "### Calculate Gradients for W1 and b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbacf337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/da1: [[0.2021    0.2273625]]\n",
      "dL/dz1: [[0.2021    0.2273625]]\n",
      "\n",
      "dL/dW1:\n",
      " [[0.10105    0.11368125]\n",
      " [0.02021    0.02273625]]\n",
      "dL/db1: [[0.2021    0.2273625]]\n"
     ]
    }
   ],
   "source": [
    "# Backpropagate error to hidden layer\n",
    "dL_da1 = np.dot(dL_dz2, W2.T)\n",
    "\n",
    "# For linear activation: da1/dz1 = 1\n",
    "dL_dz1 = dL_da1 * 1\n",
    "\n",
    "# Gradient for W1: dL/dW1 = X^T · dL/dz1\n",
    "dL_dW1 = np.dot(X.T, dL_dz1)\n",
    "\n",
    "# Gradient for b1: dL/db1 = dL/dz1\n",
    "dL_db1 = dL_dz1\n",
    "\n",
    "print(\"dL/da1:\", dL_da1)\n",
    "print(\"dL/dz1:\", dL_dz1)\n",
    "print(\"\\ndL/dW1:\\n\", dL_dW1)\n",
    "print(\"dL/db1:\", dL_db1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c000e80",
   "metadata": {},
   "source": [
    "## Step 6: Update Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE UPDATE:\n",
      "W1:\n",
      "[[0.15 0.25]\n",
      " [0.2  0.3 ]]\n",
      "b1: [[0.35 0.35]]\n",
      "W2:\n",
      "[[0.4 ]\n",
      " [0.45]]\n",
      "b2: [[0.6]]\n",
      "\n",
      "AFTER UPDATE (lr=0.5):\n",
      "W1_new:\n",
      "[[0.099475   0.19315937]\n",
      " [0.189895   0.28863187]]\n",
      "b1_new: [[0.24895    0.23631875]]\n",
      "W2_new:\n",
      "[[0.28758188]\n",
      " [0.32242437]]\n",
      "b2_new: [[0.347375]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BEFORE UPDATE:\")\n",
    "print(f\"W1:\\n{W1}\")\n",
    "print(f\"b1: {b1}\")\n",
    "print(f\"W2:\\n{W2}\")\n",
    "print(f\"b2: {b2}\")\n",
    "\n",
    "# Update rule: W = W - lr * dL/dW\n",
    "W1_new = W1 - lr * dL_dW1\n",
    "b1_new = b1 - lr * dL_db1\n",
    "W2_new = W2 - lr * dL_dW2\n",
    "b2_new = b2 - lr * dL_db2\n",
    "\n",
    "print(\"\\nAFTER UPDATE (lr=0.5):\")\n",
    "print(f\"W1_new:\\n{W1_new}\")\n",
    "print(f\"b1_new: {b1_new}\")\n",
    "print(f\"W2_new:\\n{W2_new}\")\n",
    "print(f\"b2_new: {b2_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9596b14",
   "metadata": {},
   "source": [
    "## Step 7: Forward Pass with Updated Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba15c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Prediction: 1.005250\n",
      "New Prediction: 0.555374\n",
      "Target: 0.5\n",
      "\n",
      "Old Loss: 0.127639\n",
      "New Loss: 0.001533\n",
      "Loss Reduction: 0.126106\n"
     ]
    }
   ],
   "source": [
    "# Forward pass with new weights\n",
    "z1_new = np.dot(X, W1_new) + b1_new\n",
    "a1_new = z1_new\n",
    "\n",
    "z2_new = np.dot(a1_new, W2_new) + b2_new\n",
    "a2_new = z2_new\n",
    "\n",
    "loss_new = 0.5 * (y_true - a2_new) ** 2\n",
    "\n",
    "print(f\"Old Prediction: {a2[0,0]:.6f}\")\n",
    "print(f\"New Prediction: {a2_new[0,0]:.6f}\")\n",
    "print(f\"Target: {y_true[0,0]}\")\n",
    "print(f\"\\nOld Loss: {loss[0,0]:.6f}\")\n",
    "print(f\"New Loss: {loss_new[0,0]:.6f}\")\n",
    "print(f\"Loss Reduction: {(loss[0,0] - loss_new[0,0]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e917f5",
   "metadata": {},
   "source": [
    "## Step 8: Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2562b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter  Initial  Gradient  Updated\n",
      "  W1[0,0]     0.15  0.101050 0.099475\n",
      "  W1[0,1]     0.25  0.113681 0.193159\n",
      "  W1[1,0]     0.20  0.020210 0.189895\n",
      "  W1[1,1]     0.30  0.022736 0.288632\n",
      "    b1[0]     0.35  0.202100 0.248950\n",
      "    b1[1]     0.35  0.227362 0.236319\n",
      "  W2[0,0]     0.40  0.224836 0.287582\n",
      "  W2[1,0]     0.45  0.255151 0.322424\n",
      "    b2[0]     0.60  0.505250 0.347375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Parameter': ['W1[0,0]', 'W1[0,1]', 'W1[1,0]', 'W1[1,1]', \n",
    "                  'b1[0]', 'b1[1]', 'W2[0,0]', 'W2[1,0]', 'b2[0]'],\n",
    "    'Initial': [W1[0,0], W1[0,1], W1[1,0], W1[1,1], \n",
    "                b1[0,0], b1[0,1], W2[0,0], W2[1,0], b2[0,0]],\n",
    "    'Gradient': [dL_dW1[0,0], dL_dW1[0,1], dL_dW1[1,0], dL_dW1[1,1],\n",
    "                 dL_db1[0,0], dL_db1[0,1], dL_dW2[0,0], dL_dW2[1,0], dL_db2[0,0]],\n",
    "    'Updated': [W1_new[0,0], W1_new[0,1], W1_new[1,0], W1_new[1,1],\n",
    "                b1_new[0,0], b1_new[0,1], W2_new[0,0], W2_new[1,0], b2_new[0,0]]\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30151be6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Backpropagation Steps (Manual):**\n",
    "1. Forward pass: Calculate z and a for each layer\n",
    "2. Calculate loss (MSE)\n",
    "3. Compute output layer gradients: dL/dW2, dL/db2\n",
    "4. Backpropagate to hidden layer: dL/dW1, dL/db1\n",
    "5. Update weights: W = W - lr × dL/dW\n",
    "\n",
    "**Key Formulas:**\n",
    "- MSE Loss: L = 0.5 × (y - ŷ)²\n",
    "- Linear activation derivative: da/dz = 1\n",
    "- Weight gradient: dL/dW = a^T · dL/dz\n",
    "- Backprop: dL/da₁ = dL/dz₂ · W₂^T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
